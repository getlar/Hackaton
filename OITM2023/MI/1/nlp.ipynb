{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1whg44Q1UeM6d6NMuUVND2bQ6axqB2nvM\n",
      "To: w:\\Hackaton\\Országos IT megmérettetés\\MI\\1\\text_dataset.csv\n",
      "\n",
      "  0%|          | 0.00/489k [00:00<?, ?B/s]\n",
      "100%|██████████| 489k/489k [00:00<00:00, 4.83MB/s]\n",
      "100%|██████████| 489k/489k [00:00<00:00, 4.83MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12Xp3ouuMa1U3EFv3CY5U043ycU5QVm8T\n",
      "To: w:\\Hackaton\\Országos IT megmérettetés\\MI\\1\\spamtext_classifier.h5\n",
      "\n",
      "  0%|          | 0.00/1.74M [00:00<?, ?B/s]\n",
      " 30%|███       | 524k/1.74M [00:00<00:00, 4.91MB/s]\n",
      "100%|██████████| 1.74M/1.74M [00:00<00:00, 12.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1whg44Q1UeM6d6NMuUVND2bQ6axqB2nvM\n",
    "!gdown 12Xp3ouuMa1U3EFv3CY5U043ycU5QVm8T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./text_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={'legit':0, 'spam':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['input'] = data['text'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "data['input'] = data['input'].apply(lambda x: x.replace('\\u200a',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\n",
    "tokenizer.fit_on_texts(data['input'])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "data['input']=data['input'].apply(lambda x: tokenizer.texts_to_sequences([x])[0])\n",
    "max_sequence_len = data['input'].str.len().max()\n",
    "print(max_sequence_len)\n",
    "data[\"input\"]=data[\"input\"].apply(lambda x: np.array(pad_sequences([x], maxlen=max_sequence_len, padding='pre')))\n",
    "data[\"input\"]=data[\"input\"].apply(lambda x: x.squeeze())\n",
    "data[\"label\"]=data['category'].apply(lambda x: label_dict[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ds = tf.data.Dataset.from_tensor_slices((data[\"input\"].tolist(), to_categorical(data[\"label\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tktm_\\AppData\\Local\\Temp\\ipykernel_15572\\3977084260.py:20: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n",
      "c:\\Users\\tktm_\\anaconda3\\envs\\T\\lib\\site-packages\\keras\\backend.py:5531: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 1s 4ms/step - loss: 0.4763 - categorical_accuracy: 0.8643\n",
      "Epoch 2/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.3728 - categorical_accuracy: 0.8659\n",
      "Epoch 3/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.3622 - categorical_accuracy: 0.8659\n",
      "Epoch 4/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.3451 - categorical_accuracy: 0.8659\n",
      "Epoch 5/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.3269 - categorical_accuracy: 0.8659\n",
      "Epoch 6/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.3051 - categorical_accuracy: 0.8659\n",
      "Epoch 7/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.2772 - categorical_accuracy: 0.8661\n",
      "Epoch 8/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.2464 - categorical_accuracy: 0.8708\n",
      "Epoch 9/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.2134 - categorical_accuracy: 0.9017\n",
      "Epoch 10/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.1832 - categorical_accuracy: 0.9307\n",
      "Epoch 11/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.1570 - categorical_accuracy: 0.9512\n",
      "Epoch 12/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.1359 - categorical_accuracy: 0.9628\n",
      "Epoch 13/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.1196 - categorical_accuracy: 0.9684\n",
      "Epoch 14/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.1052 - categorical_accuracy: 0.9720\n",
      "Epoch 15/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0935 - categorical_accuracy: 0.9745\n",
      "Epoch 16/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0853 - categorical_accuracy: 0.9777\n",
      "Epoch 17/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0774 - categorical_accuracy: 0.9795\n",
      "Epoch 18/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0711 - categorical_accuracy: 0.9815\n",
      "Epoch 19/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0662 - categorical_accuracy: 0.9822\n",
      "Epoch 20/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0612 - categorical_accuracy: 0.9833\n",
      "Epoch 21/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0565 - categorical_accuracy: 0.9844\n",
      "Epoch 22/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0542 - categorical_accuracy: 0.9855\n",
      "Epoch 23/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0497 - categorical_accuracy: 0.9858\n",
      "Epoch 24/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0475 - categorical_accuracy: 0.9871\n",
      "Epoch 25/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0448 - categorical_accuracy: 0.9880\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(total_words + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model.compile(loss=losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.CategoricalAccuracy())\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "input_batches = input_ds.shuffle(1000).batch(BATCH_SIZE)\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "    input_batches,\n",
    "    epochs=epochs)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "legit\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Guess what? You just won a vacation to the Maldives! Cant call you right now, because im busy, but pls venmo me 100 dollars to deposit the prize, and thats it! We dont have to pay more! i will call you tomorrow first thing in the morning!\"\n",
    "sentence = sentence.lower() # lowercase the input sentence\n",
    "sequence = tokenizer.texts_to_sequences([sentence])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_sequence_len, padding='pre')\n",
    "pred = model.predict(padded_sequence)\n",
    "probabilities = np.exp(pred) / np.sum(np.exp(pred), axis=0)\n",
    "\n",
    "predicted_class = np.argmax(probabilities)\n",
    "\n",
    "class_labels = {0: 'legit', 1: 'spam'}\n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "print(predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
